import { db } from "../db";
import { challengeSubmissions } from "@shared/schema";
import { eq } from "drizzle-orm";
import { metrics } from "./metrics";
import { awardPoints, applyFailurePenalty, type GamificationResult } from "./gamification";
import type { Message, TextChannel } from "discord.js";

// ‚îÄ‚îÄ‚îÄ Types ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

export interface ChallengeInfo {
    id: string;
    title: string;
    description: string;
    solution: string;
    tags: string[];
}

export interface AIReviewResult {
    isCorrect: boolean;
    confidence: number;
    explanation: string;
    aiGeneratedConfidence: number;  // 0-1 ‚Äî how likely the code was AI-generated
    aiGeneratedReason: string;
}

export interface ReviewJob {
    submissionId: number;
    message: Message;
    challenge: ChallengeInfo;
    attemptNumber: number;
    totalAttempts: number; // how many attempts the user has used (including this one)
    detectedLanguage: string;
    userCode: string;
    userId: bigint;
    guildId: bigint;
}

// Language Detection

const LANG_MAP: Record<string, string> = {
    js: "JavaScript", javascript: "JavaScript", jsx: "JavaScript",
    ts: "TypeScript", typescript: "TypeScript", tsx: "TypeScript",
    py: "Python", python: "Python",
    java: "Java",
    c: "C", cpp: "C++", "c++": "C++", csharp: "C#", "c#": "C#", cs: "C#",
    rb: "Ruby", ruby: "Ruby",
    go: "Go", golang: "Go",
    rs: "Rust", rust: "Rust",
    php: "PHP",
    swift: "Swift",
    kt: "Kotlin", kotlin: "Kotlin",
    scala: "Scala",
    lua: "Lua",
    r: "R",
    dart: "Dart",
    sh: "Shell", bash: "Shell", zsh: "Shell",
};

/**
 * Extract code from Discord message content.
 * Returns { language, code } ‚Äî language defaults to "Unknown" if no fence tag.
 */
export function extractCode(content: string): { language: string; code: string } | null {
    // Match ```lang\ncode``` or ```\ncode```
    const fenceMatch = content.match(/```(\w*)\n([\s\S]*?)```/);
    if (fenceMatch) {
        const rawLang = fenceMatch[1]?.toLowerCase() || "";
        const language = LANG_MAP[rawLang] || (rawLang ? rawLang.charAt(0).toUpperCase() + rawLang.slice(1) : "Unknown");
        return { language, code: fenceMatch[2].trim() };
    }

    // Inline code block (single backticks with substantial code)
    const inlineMatch = content.match(/`([^`]{10,})`/);
    if (inlineMatch) {
        return { language: "Unknown", code: inlineMatch[1].trim() };
    }

    return null;
}

// AI Call

const AI_TIMEOUT_MS = 30_000;
const MAX_RETRIES = 2;
const BASE_BACKOFF_MS = 1_000;

// Circuit breaker: if we fail X times in a row, stop calling AI for a cooldown
let consecutiveFailures = 0;
const CIRCUIT_BREAKER_THRESHOLD = 5;
const CIRCUIT_BREAKER_COOLDOWN_MS = 60_000;
let circuitBreakerOpenUntil = 0;

function buildPrompt(challenge: ChallengeInfo, userCode: string, attemptNumber: number, detectedLanguage: string): string {
    return `You are a strict coding challenge reviewer AND an AI-generated code detector.
You have TWO jobs:

JOB 1 ‚Äî CORRECTNESS REVIEW:
The reference solution is written in JavaScript.
The user's solution is written in ${detectedLanguage}.
Determine if they solve the same problem logically, regardless of syntax differences.
Ignore variable names, whitespace, and language idioms. Focus only on logical correctness.

JOB 2 ‚Äî AI-GENERATED CODE DETECTION (BE STRICT):
Analyse whether this code was likely generated by an AI (ChatGPT, Copilot, Claude, etc.).
Look for these specific patterns:
- Overly descriptive variable names that read like documentation (inputArray, resultString, isValidInput)
- Excessive or unnecessary comments explaining obvious operations (// loop through array, // return result)
- Boilerplate-heavy structure (full JSDoc on simple functions, unnecessary type annotations)
- ChatGPT signature patterns: always using const, arrow functions everywhere, over-abstracting simple logic
- Suspiciously "textbook-perfect" code structure with perfect error handling for a simple challenge
- Code that looks "cleaned up" from AI output ‚Äî comments removed but robotic structure remains
- Unnaturally consistent formatting and naming conventions throughout
- Solutions that look like they came from a coding tutorial or Stack Overflow answer verbatim
- Using advanced patterns or abstractions that are unnecessary for the problem's complexity
Human-written code in Discord challenges is usually rough, minimal, sometimes messy ‚Äî that's NORMAL.
If the code looks too polished or structured for a casual Discord challenge submission, flag it.

Challenge: ${challenge.title}
Reference Solution (JavaScript): ${challenge.solution}
User Attempt #${attemptNumber} (${detectedLanguage}): ${userCode}

Return ONLY valid JSON, no markdown, no explanation outside JSON:
{"isCorrect": boolean, "confidence": number, "explanation": "string", "aiGeneratedConfidence": number, "aiGeneratedReason": "string"}

Rules for aiGeneratedConfidence:
- 0.0 = definitely human-written
- 0.5 = unsure
- 0.75+ = likely AI-generated (will be REJECTED)
- 1.0 = obviously AI-generated
Be strict. When in doubt, lean towards flagging.`;
}

async function callAI(prompt: string): Promise<AIReviewResult> {
    // Check circuit breaker
    if (Date.now() < circuitBreakerOpenUntil) {
        throw new Error("Circuit breaker open ‚Äî AI calls paused");
    }

    const aiEndpoint = process.env.AI_ENDPOINT || "http://localhost:11434";
    let lastError: Error | null = null;

    for (let attempt = 0; attempt <= MAX_RETRIES; attempt++) {
        if (attempt > 0) {
            // Exponential backoff
            const delay = BASE_BACKOFF_MS * Math.pow(2, attempt - 1);
            await new Promise((r) => setTimeout(r, delay));
        }

        const startTime = Date.now();
        let failed = false;

        try {
            const controller = new AbortController();
            const timeout = setTimeout(() => controller.abort(), AI_TIMEOUT_MS);

            const res = await fetch(`${aiEndpoint}/api/generate`, {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify({
                    model: "llama3:8b",
                    system: "You are a code review assistant. You ONLY respond with valid JSON. No markdown. No extra text.",
                    prompt,
                    stream: false,
                    keep_alive: "10m",
                    options: { num_ctx: 2048 },
                }),
                signal: controller.signal,
            });

            clearTimeout(timeout);

            if (!res.ok) {
                throw new Error(`AI HTTP ${res.status}`);
            }

            const data: any = await res.json();
            const rawResponse = data.response || "";

            // Parse JSON from response (handle possible markdown wrapping)
            const jsonStr = rawResponse.replace(/```json?\n?/g, "").replace(/```/g, "").trim();
            const parsed = JSON.parse(jsonStr);

            // Validate shape
            if (typeof parsed.isCorrect !== "boolean" || typeof parsed.confidence !== "number" || typeof parsed.explanation !== "string") {
                throw new Error("Invalid AI response shape");
            }

            const latency = Date.now() - startTime;
            metrics.recordAICall(latency, false);
            consecutiveFailures = 0;

            return {
                isCorrect: parsed.isCorrect,
                confidence: Math.max(0, Math.min(1, parsed.confidence)),
                explanation: parsed.explanation.slice(0, 1000), // Cap length
                aiGeneratedConfidence: Math.max(0, Math.min(1, parsed.aiGeneratedConfidence ?? 0)),
                aiGeneratedReason: (parsed.aiGeneratedReason || "No reason provided.").slice(0, 500),
            };
        } catch (err: any) {
            failed = true;
            lastError = err;
            const latency = Date.now() - startTime;
            metrics.recordAICall(latency, true);
        }
    }

    // All retries exhausted
    consecutiveFailures++;
    if (consecutiveFailures >= CIRCUIT_BREAKER_THRESHOLD) {
        circuitBreakerOpenUntil = Date.now() + CIRCUIT_BREAKER_COOLDOWN_MS;
        console.error(`[Challenge Reviewer] Circuit breaker OPEN ‚Äî ${consecutiveFailures} consecutive failures. Pausing for ${CIRCUIT_BREAKER_COOLDOWN_MS / 1000}s.`);
    }

    throw lastError || new Error("AI call failed after retries");
}

// Review Processing

/**
 * Process a single review job. Called by the queue worker.
 * Updates DB state and replies in the thread.
 */
export async function processReviewJob(job: ReviewJob): Promise<void> {
    const { submissionId, message, challenge, attemptNumber, totalAttempts, detectedLanguage, userCode, userId, guildId } = job;

    try {
        // 1. Mark as REVIEWING
        await db
            .update(challengeSubmissions)
            .set({ reviewState: "REVIEWING", reviewStartedAt: new Date() })
            .where(eq(challengeSubmissions.id, submissionId));

        // 2. Call AI
        const prompt = buildPrompt(challenge, userCode, attemptNumber, detectedLanguage);
        const result = await callAI(prompt);

        // 2.5 AI-generated code check ‚Äî reject if flagged
        if (result.aiGeneratedConfidence >= 0.75) {
            await db
                .update(challengeSubmissions)
                .set({
                    reviewState: "REVIEWED",
                    status: "INCORRECT",
                    aiConfidence: result.aiGeneratedConfidence,
                    aiExplanation: `[AI-GENERATED] ${result.aiGeneratedReason}`,
                })
                .where(eq(challengeSubmissions.id, submissionId));

            metrics.recordReview(result.aiGeneratedConfidence);

            const rejectMsg = [
                `## AI-Generated Code Detected (Attempt ${attemptNumber}/3)`,
                "",
                result.aiGeneratedReason,
                "",
                "Write your own code ‚Äî you don't get a job copying from ChatGPT.",
                "This attempt has been counted.",
            ].join("\n");

            await message.reply(rejectMsg).catch((e) => console.error("[Challenge Reviewer] Failed to reply:", e));
            await sendAuditLog(job, "INCORRECT", result, null, null, true);
            return;
        }

        // 3. Determine status
        let status: "CORRECT" | "INCORRECT" | "PARTIAL";
        if (result.isCorrect && result.confidence >= 0.6) {
            status = "CORRECT";
        } else if (result.isCorrect && result.confidence < 0.6) {
            status = "PARTIAL";
        } else {
            status = "INCORRECT";
        }

        // 4. Update submission
        await db
            .update(challengeSubmissions)
            .set({
                reviewState: "REVIEWED",
                status,
                aiConfidence: result.confidence,
                aiExplanation: result.explanation,
            })
            .where(eq(challengeSubmissions.id, submissionId));

        metrics.recordReview(result.confidence);

        // 5. Handle gamification
        let gamResult: GamificationResult | null = null;
        let penaltyInfo: { pointsDeducted: number; totalPoints: number } | null = null;

        if (status === "CORRECT") {
            gamResult = await awardPoints(userId, guildId, submissionId, attemptNumber);
        } else if (status === "INCORRECT" && attemptNumber >= 3) {
            // All 3 attempts failed
            penaltyInfo = await applyFailurePenalty(userId, guildId);
        }

        // 6. Reply in thread
        const reply = buildReplyMessage(status, result, attemptNumber, totalAttempts, gamResult, penaltyInfo);
        await message.reply(reply).catch((e) => console.error("[Challenge Reviewer] Failed to reply:", e));

        // 7. Send audit log
        await sendAuditLog(job, status, result, gamResult, penaltyInfo, false);
    } catch (err) {
        console.error(`[Challenge Reviewer] Failed to process submission ${submissionId}:`, err);

        // Mark as FAILED
        await db
            .update(challengeSubmissions)
            .set({ reviewState: "FAILED" })
            .where(eq(challengeSubmissions.id, submissionId))
            .catch((e) => console.error("[Challenge Reviewer] Failed to mark FAILED:", e));

        // Notify user of failure
        await message
            .reply(`Sorry, I couldn't review your submission right now. It'll be retried when I restart. (Attempt ${attemptNumber}/3)`)
            .catch(() => { });
    }
}

// Reply Builder

function buildReplyMessage(
    status: "CORRECT" | "INCORRECT" | "PARTIAL",
    result: AIReviewResult,
    attemptNumber: number,
    totalAttempts: number,
    gamResult: GamificationResult | null,
    penaltyInfo: { pointsDeducted: number; totalPoints: number } | null,
): string {
    const lines: string[] = [];

    // Status header
    if (status === "CORRECT") {
        lines.push(`## ‚úÖ Correct! (Attempt ${attemptNumber}/3)`);
    } else if (status === "PARTIAL") {
        lines.push(`## üü° Partially Correct (Attempt ${attemptNumber}/3)`);
        lines.push(`> Confidence: ${(result.confidence * 100).toFixed(0)}% ‚Äî the AI isn't fully confident. You may want to refine your approach.`);
    } else {
        lines.push(`## ‚ùå Incorrect (Attempt ${attemptNumber}/3)`);
    }

    // AI explanation
    lines.push("");
    lines.push(result.explanation);

    // Gamification
    if (gamResult) {
        lines.push("");
        lines.push(`**+${gamResult.pointsAwarded} points** | Total: ${gamResult.totalPoints} | Streak: ${gamResult.currentStreak}`);
    }

    if (penaltyInfo && penaltyInfo.pointsDeducted > 0) {
        lines.push("");
        lines.push(`**-${penaltyInfo.pointsDeducted} points** | Total: ${penaltyInfo.totalPoints} | Streak reset`);
    }

    // Remaining attempts
    if (status !== "CORRECT" && attemptNumber < 3) {
        const remaining = 3 - attemptNumber;
        lines.push("");
        lines.push(`üí° You have **${remaining}** attempt${remaining > 1 ? "s" : ""} remaining. Try again!`);
    } else if (status !== "CORRECT" && attemptNumber >= 3) {
        lines.push("");
        lines.push("You've used all 3 attempts for this challenge. Better luck next time!");
    }

    return lines.join("\n");
}

// Audit Log

let _auditLogChannel: TextChannel | null = null;

export function setAuditLogChannel(channel: TextChannel | null): void {
    _auditLogChannel = channel;
}

async function sendAuditLog(
    job: ReviewJob,
    status: "CORRECT" | "INCORRECT" | "PARTIAL",
    result: AIReviewResult,
    gamResult: GamificationResult | null,
    penaltyInfo: { pointsDeducted: number; totalPoints: number } | null,
    aiDetected: boolean = false,
): Promise<void> {
    if (!_auditLogChannel) return;

    const statusEmoji = aiDetected ? "ü§ñ" : status === "CORRECT" ? "‚úÖ" : status === "PARTIAL" ? "üü°" : "‚ùå";
    const pointsStr = gamResult
        ? `+${gamResult.pointsAwarded}`
        : penaltyInfo
            ? `-${penaltyInfo.pointsDeducted}`
            : "0";

    const logLines = [
        `**Review Log**`,
        `‚îú‚îÄ User: <@${job.userId}> (Attempt ${job.attemptNumber}/3)`,
        `‚îú‚îÄ Challenge: [${job.challenge.id}] ${job.challenge.title}`,
        `‚îú‚îÄ Language: ${job.detectedLanguage}`,
        `‚îú‚îÄ Status: ${statusEmoji} ${aiDetected ? "AI-GENERATED" : status}`,
        `‚îú‚îÄ Confidence: ${(result.confidence * 100).toFixed(0)}%`,
    ];

    if (aiDetected) {
        logLines.push(`‚îú‚îÄ AI Detection: ${(result.aiGeneratedConfidence * 100).toFixed(0)}% ‚Äî ${result.aiGeneratedReason.slice(0, 150)}`);
    }

    logLines.push(
        `‚îú‚îÄ AI Explanation: "${result.explanation.slice(0, 200)}"`,
        `‚îú‚îÄ Points: ${pointsStr}`,
        `‚îî‚îÄ Thread: <#${job.message.channel.id}>`,
    );

    await _auditLogChannel.send(logLines.join("\n")).catch((e) => console.error("[Audit Log] Failed to send:", e));
}
